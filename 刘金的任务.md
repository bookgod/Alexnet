#Alexnet 学习笔记
- ReLU非线性不饱和函数的在训练时间上比tanh(x）等非线性饱和函数快很多。在数据量大是是很好的选择。
- Alexnet利用了典型的以空间换时间的思想，gpu并行加速。
- 掉线机制：每轮迭代时，网络中每个隐含层的输出节点都有一半的可能性被置为0，使其掉线不参与此轮迭代，包括前向传播与反向传播。如此一来，我们每一轮的训练都是在不同架构的网络下进行的，并且还实现权值的共享.
- Alexnet组成：有6000万个参数650000个神经元。
有五个卷积层，其中有些后面跟着池化层。三个最终有1000-way的softmax的全连接层。![](https://img-blog.csdn.net/20170628172609574?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvT2xpdmVya2luZ0xp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)![](https://img-blog.csdn.net/20170628160819797?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvT2xpdmVya2luZ0xp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)